{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7417f207",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "\n",
    "## Data Acquisition  ->  Text Extraction & Cleanup  ->  PreProcessing  ->  Feature Engineering  ->  Model Building  ->  Evaluation  -> Deployment  -> Monitor & Update\n",
    "\n",
    "#### PreProcessing consists of Tokenization , Stemming and Lemmatization.\n",
    "\n",
    "#### Tokenization consists of sperating/segmentation sentences if sentence tokenizaityon and segmenting words if word tokenization\n",
    "\n",
    "### Tokeinzation is not easy as it sounds , you cant really use sperate a given para at '.' we will see why ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad911637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maybe\n",
      "getting\n",
      "a\n",
      "Dr.\n",
      "Degree\n",
      "through\n",
      "PhD\n",
      "not\n",
      "that\n",
      "valuable\n",
      "as\n",
      "getting\n",
      "it\n",
      "through\n",
      "M.B.B.S\n",
      "atleast\n",
      "in\n",
      "India\n",
      ".\n",
      "Also\n",
      "mbbs\n",
      "people\n",
      "get\n",
      "paid\n",
      "more\n",
      "and\n",
      "they\n",
      "have\n",
      "so\n",
      "many\n",
      "buisness\n",
      "prospects\n",
      "too\n",
      ".\n",
      "While\n",
      "the\n",
      "PhD\n",
      "they\n",
      "paid\n",
      "less\n",
      "and\n",
      "no\n",
      "respect\n",
      "and\n",
      "not\n",
      "much\n",
      "buisness\n",
      "either\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Explore Spacy Documentation for a bigger project here just experimenting\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Maybe getting a Dr. Degree through PhD not that valuable as getting it through M.B.B.S atleast in India. Also mbbs people get paid more and they have so many buisness prospects too. While the PhD they paid less and no respect and not much buisness either.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd41208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]\n",
    "doc[1]\n",
    "doc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c37ff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42a9e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b048920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[1:5]\n",
    "type(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e14256b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Defaults',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_components',\n",
       " '_config',\n",
       " '_disabled',\n",
       " '_ensure_doc',\n",
       " '_ensure_doc_with_context',\n",
       " '_factory_meta',\n",
       " '_get_pipe_index',\n",
       " '_has_gpu_model',\n",
       " '_link_components',\n",
       " '_meta',\n",
       " '_multiprocessing_pipe',\n",
       " '_optimizer',\n",
       " '_path',\n",
       " '_pipe_configs',\n",
       " '_pipe_meta',\n",
       " '_resolve_component_status',\n",
       " 'add_pipe',\n",
       " 'analyze_pipes',\n",
       " 'batch_size',\n",
       " 'begin_training',\n",
       " 'component',\n",
       " 'component_names',\n",
       " 'components',\n",
       " 'config',\n",
       " 'create_optimizer',\n",
       " 'create_pipe',\n",
       " 'create_pipe_from_source',\n",
       " 'default_config',\n",
       " 'default_error_handler',\n",
       " 'disable_pipe',\n",
       " 'disable_pipes',\n",
       " 'disabled',\n",
       " 'enable_pipe',\n",
       " 'evaluate',\n",
       " 'factories',\n",
       " 'factory',\n",
       " 'factory_names',\n",
       " 'from_bytes',\n",
       " 'from_config',\n",
       " 'from_disk',\n",
       " 'get_factory_meta',\n",
       " 'get_factory_name',\n",
       " 'get_pipe',\n",
       " 'get_pipe_config',\n",
       " 'get_pipe_meta',\n",
       " 'has_factory',\n",
       " 'has_pipe',\n",
       " 'initialize',\n",
       " 'lang',\n",
       " 'make_doc',\n",
       " 'max_length',\n",
       " 'memory_zone',\n",
       " 'meta',\n",
       " 'path',\n",
       " 'pipe',\n",
       " 'pipe_factories',\n",
       " 'pipe_labels',\n",
       " 'pipe_names',\n",
       " 'pipeline',\n",
       " 'rehearse',\n",
       " 'remove_pipe',\n",
       " 'rename_pipe',\n",
       " 'replace_listeners',\n",
       " 'replace_pipe',\n",
       " 'resume_training',\n",
       " 'select_pipes',\n",
       " 'set_error_handler',\n",
       " 'set_factory_meta',\n",
       " 'to_bytes',\n",
       " 'to_disk',\n",
       " 'tokenizer',\n",
       " 'update',\n",
       " 'use_params',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d653c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_context',\n",
       " '_get_array_attrs',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'from_json',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cd2b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95610566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23aba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.text\n",
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f08d94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ===>> index: 0, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 1, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 2, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 3, is_aplha: False, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 4, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 5, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 6, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 7, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 8, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 9, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 10, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 11, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 12, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 13, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 14, is_aplha: False, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 15, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 16, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 17, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 18, is_aplha: False, is_punct: True, like_num: False, is_currency: False\n",
      "token ===>> index: 19, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 20, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 21, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 22, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 23, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 24, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 25, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 26, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 27, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 28, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 29, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 30, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 31, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 32, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 33, is_aplha: False, is_punct: True, like_num: False, is_currency: False\n",
      "token ===>> index: 34, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 35, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 36, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 37, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 38, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 39, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 40, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 41, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 42, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 43, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 44, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 45, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 46, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 47, is_aplha: True, is_punct: False, like_num: False, is_currency: False\n",
      "token ===>> index: 48, is_aplha: False, is_punct: True, like_num: False, is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"token ===>> index: {token.i}, is_aplha: {token.is_alpha}, is_punct: {token.is_punct}, like_num: {token.like_num}, is_currency: {token.is_currency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67251feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ac3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e638a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
