{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be910b3b",
   "metadata": {},
   "source": [
    "### Stemming :- Using fixed rules to reduce the given word to its base word such as removing able, ing etc. from the end of given word is called stemming. Here you dont need the knowledge of the langugage. for example talking -> talk, remarkable -> remark, ability -> abil(WRONG)\n",
    "\n",
    "### Lemmatization:- Here you will need linguistic knowledge of the language to conver the word to its base knowledge. For example ate -> eat, ability -> ability\n",
    "\n",
    "##### NLTK has support for both stemming and lemmatization but the spacy doesnt have support for stemming. So we will be using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee86889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a3b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b5bb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "ability | abil\n",
      "eats | eat\n",
      "ate | ate\n",
      "rafting | raft\n",
      "adjustable | adjust\n",
      "rafting | raft\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\",\"ability\",\"eats\", \"ate\", \"rafting\", \"adjustable\",\"rafting\"]\n",
    "for word in words:\n",
    "    print(f\"{word} | {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8534ea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eating | eat\n",
      "ate | eat\n",
      "adjustable | adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meet\n",
      "better | well\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"eating eats eating ate adjustable rafting ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token} | {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344886a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8e0163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let | let\n",
      "'s | us\n",
      "go | go\n",
      "bro | bro\n",
      "! | !\n",
      "Bruh | bruh\n",
      ", | ,\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n",
      ", | ,\n",
      "so | so\n",
      "probabbly | probabbly\n",
      "tomorrow | tomorrow\n",
      ". | .\n"
     ]
    }
   ],
   "source": [
    "text = \"Let's go bro! Bruh, I am exhausted, so probabbly tomorrow.\"\n",
    "doc1 = nlp(text)\n",
    "\n",
    "for t in doc1:\n",
    "    print(t.text, \"|\", t.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d69accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let  |  let\n",
      "'s  |  us\n",
      "go  |  go\n",
      "bro  |  Brother\n",
      "!  |  !\n",
      "Bruh  |  Brother\n",
      ",  |  ,\n",
      "I  |  I\n",
      "am  |  be\n",
      "exhausted  |  exhaust\n",
      ",  |  ,\n",
      "so  |  so\n",
      "probabbly  |  probabbly\n",
      "tomorrow  |  tomorrow\n",
      ".  |  .\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "ar.add([[{\"TEXT\": \"bro\"}], [{\"TEXT\":\"Bruh\"}]], {\"LEMMA\": \"Brother\"})\n",
    "text = \"Let's go bro! Bruh, I am exhausted, so probabbly tomorrow.\"\n",
    "doc1 = nlp(text)\n",
    "for t in doc1:\n",
    "    print(t.text, \" | \", t.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d49f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a86791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f719e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749064d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
